\section{Немного теории}
\subsection{Искусственная нейронная сеть}

	\textit{Искусственная нейронная сеть} --- математическая модель, построенная по принципу организации биологических нейронных сетей.
	На вход ИНС подаётся некоторое множество параметров (называемых \textit{предикторами}), определяющих конкретный пример.
	На выходе ИНС выдаёт предсказание, к какому классу принадлежит данный пример.

	В качестве модели ИНС используется \textit{перцептрон Розенблатта}.
	Она состоит из нескольких слоёв нейронов:
	\begin{enumerate}
		\item Входной слой, содержащий псевдо-нейроны, которые передают дальше значения предикторов;
		\item Один или несколько скрытых слоёв;
		\item Выходной слой, содержащий один нейрон.
	\end{enumerate}
	Передача сигналов (активация) нейронной сети происходит от входного слоя, через скрытые слои, к выходному слою.

	\includegraphics{net.png}

	Все нейроны (кроме входного слоя) имеют одинаковое строение, состоят из двух частей --- сумматорной и активационной функций.
	Сумматорная функция определяет то, как нейрон будет использовать входящую информацию.
	Активационная функция определяет реакцию нейрона, которая будет передана на все выходы.
	Каждый нейрон из скрытых или выходного слоёв получает на вход значения активационных функций всех нейронов предыдущего слоя.

	В качестве сумматорной функции выбрана взвешенная сумма всех входящих сигналов:
	\[
		S = b + \sum_{j = 1}^m x_j w_j
	\]
	где $m$ --- количество входящих сигналов нейрона, $x_j$ --- значение $j$-ого входящего сигнала, $w_j$ --- вес $j$-ого входа,
	$b$ --- некоторое смещение, изменяемое в процессе обучения.

	Активационная функция --- логистическая (сигмоидальная):
	\[
		\sigma\left(S\right) = \frac{1}{1 + e^{-S}}
	\]

	\includegraphics{sigmoid.png}

	Здесь стоит сразу сказать, что смещение можно учитывать в сумме, если добавить в каждый слой, кроме выходного
	на первое место нейрон, у которого значение активации будет всегда равно 1.

	\textit{Обучение нейронной сети} --- это настройка весов для входов всех нейронов, с целью получения достоверных предсказаний.
	Обучение проводится на обучающей выборке --- множестве примеров с известными правильными ответами.
	Для обучения используется \textit{алгоритм обратного распространенния ошибки}.

	Для оценки правдоподобности предсказаний используется \textit{квадратичная функция ошибки}:
	\[
		E = \frac{1}{2N}\sum_{i = 1}^N \left(\hat y_i - y_i\right)^2
	\]
	где $N$ --- количество примеров, $\hat y_i$ --- предсказанное значение для $i$-ого примера, $y_i$ --- правильный ответ для него.

	Чтобы предсказания были как можно более точными, будем уменьшать значение функции ошибки, изменяя веса.
	Алгоритм обратного распространения ошибки основывается на градиентном спуске по пространству весов в сторону уменьшения
	значений функции ошибки.

	Для того, чтобы понять, как изменится значение функции ошибки при изменении какого-либо веса входящих сигналов нейрона,
	нужно взять её частную производную по этому весу.
	Сначала считается изменение весов в выходном слое:
	\[
		\Delta w_j = -\alpha\frac{\partial E}{\partial w_j}
	\]
	где $\alpha \in \mathbb R$ --- скорость обучения.

	В векторном виде:
	\[
		\Delta W = -\alpha\nabla E\left(W\right)
	\]
	где $\nabla E\left(W\right) = \left(\frac{\partial E}{\partial w_1},\dots,\frac{\partial E}{\partial w_m}\right)$ --- градиент $E$ в точке $W$.

	Посчитаем частную производную от функции $E$ по $j$-му весу:
	\[
		\frac{\partial E}{\partial w_j}	= \frac{\partial\left(\frac{1}{2N}\sum_{i = 1}^N \left(\hat y_i - y_i\right)^2\right)}{\partial w_j}
	\]
	
	Так как производная суммы равна сумме производных, возьмём для простоты один пример, а после просуммируем все значения:
	\[
		\frac{1}{2}\cdot\frac{\partial{\left(\hat y - y\right)^2}}{\partial w_j} =
		\frac{1}{2}\cdot\frac{\partial\left(\hat y - y\right)^2}{\partial\hat y}\cdot\frac{\partial\hat y}{\partial w_j} =
		\left(\hat y - y\right)\frac{\partial\sigma\left(S\right)}{\partial w_j} =
	\]
	\[
		= \left(\sigma\left(S\right) - y\right)\sigma\prime\left(S\right)\frac{\partial\sum_{j = 1}^m x_j w_j}{\partial w_j} =
		\left(\sigma\left(S\right) - y\right)\sigma\left(S\right)\left(1 - \sigma\left(S\right)\right) x_j
	\]

	Итак, общая формула для $j$-ого веса по $N$ примерам:
	\[
		\frac{\partial E}{\partial w_j} = \frac{1}{N}\sum_{i = 1}^N
		\left(\hat y_i - y_i\right)\hat y_i\left(1 - \hat y_i\right) x_j
	\]

	Теперь полученная ошибка распространяется по ИНС в обратном порядке, от выходного слоя ко входному, изменяя веса скрытых слоёв.
	
	Введём следующие обозначения:
	\begin{itemize}
		\item $w_{jk}^l$ --- значение $j$-ого веса $k$-ого нейрона в $l$-ом слое (вес ребра из $j$-ого нейрона $l - 1$ слоя в $k$-ый нейрон $l$-ого слоя);
		\item $m_l$ --- количество нейронов в $l$-ом слое;
		\item $s_k^l$ --- значение сумматорной функции $k$-ого нейрона в $l$-ом слое;
		\item $a_k^l$ --- значение активационной функции $k$-ого нейрона в $l$-ом слое;
		\item $\delta_k^l = \frac{\partial E}{\partial s_k^l}$ --- ошибка $k$-ого нейрона в $l$-ом слое.
	\end{itemize}

	Зная значение ошибки $\delta_k^l$ для каждого нейрона, можно получить соответствующее изменение его весов:
	\[
		\Delta w_{jk}^l = -\alpha\delta_k^l a_j^{l-1}
	\]

	Посчитаем значение ошибки для нейронов выходного ($L$-ого) слоя. Для простоты возьмём один пример:
	\[
		\delta_k^L = \frac{\partial E}{\partial s_k^L} =
		\frac{1}{2}\cdot\frac{\partial\left(\sigma\left(s_k^L\right) - y_k\right)^2}{\partial s_k^L} = 
		\left(a_k^L - y_k\right)a_k^L\left(1 - a_k^L\right)
	\]
	где $y_k$ --- правильный ответ для $k$-ого нейрона выходного слоя.

	Теперь выразим ошибку нейрона на $l$-ом слое через ошибки на $l + 1$ слое:
	\[
		\delta_j^l = \sigma\prime\left(s_j^l\right)\sum_{k = 1}^{m_{l + 1}} w_{jk}^{l + 1} \delta_k^{l+1} = 
		a_j^l\left(1 - a_j^l\right)\sum_{k = 1}^{m_{l + 1}} w_{jk}^{l + 1} \delta_k^{l+1}
	\]

\subsection{Получение предикторов из звука}
	В качестве предикторов из звука были выбраны частоты, которые имеют наивысшую интенсивность.
	Для того, чтобы из wav-файлов получить такие предикторы, массив записей интенсивности сигнала по времени
	преобразуется с помощью быстрого дискретного преобразования Фурье в спектр --- массив интенсивности частот.

	\newpage
	\parskip=0cm

	Пример спектра записи звука целого мяча:\\
	\includegraphics{whole.png}

	Пример спектра записи звука повреждённого мяча:\\
	\includegraphics{broken.png}

	\parskip=0.2cm